# Base configuration that all experiments inherit from
# Experiment: 10fps with hand mask
experiment:
  experiment_name: "hand_mask_prediction_using_relative_ee_joint_fingers"
  experiment_description: "Predicting hand mask using relative end-effector and joint fingers"
  base_config: "../base_config.yaml"  # Reference to base config
  
# Override base config parameters
training:
  tag: "hand_mask_prediction_using_relative_ee_joint_fingers"
  output_dir: "model_ckpt/${training.tag}"
  wandb_run_name: "${experiment.experiment_name}_${training.tag}"
  wandb_project_name: "hand_mask_prediction_tests"
  
  debug: false
  learning_rate: 1e-5
  gradient_accumulation_steps: 1
  mixed_precision: "fp16"
  train_batch_size: 4
  shuffle: true
  num_train_epochs: 100
  max_train_steps: 100000
  checkpointing_steps: 10000
  validation_steps: 2500
  max_grad_norm: 1.0

  video_num: 4
  num_validation_batch: 2


dataset:
  dataset_root_path: "datasets/2026-01-16T15-34-02"
  dataset_names: "data_working_hand_mask_sine_hand_motions_10fps"
  dataset_meta_info_path: "dataset_meta_info"
  dataset_cfgs: ${dataset.dataset_names}  # Reference from dataset section
  annotation_name: "annotation"
  prob: [1.0]  # Use only one dataset
  original_fps: 50
  fps: 10
  down_sample: 5  # 50/10 = 5
  skip_step: 1
  num_workers: 4
  max_num_samples: 13000
  use_hand_mask: false
  use_only_hand_actions: false
  use_only_ee_pose_actions: false
  use_average_scalar_hand_action: false

  predicted_datatype: "latent_segmentation_videos" # "latent_videos" or "latent_segmentation_videos" or "rgb" or "segmentation" or "depth" or "optical_flow"

model:
  # Model paths
  svd_model_path: "stabilityai/stable-video-diffusion-img2vid"
  clip_model_path: "openai/clip-vit-base-patch32"
  ckpt_path: null
  pi_ckpt: null  # Policy checkpoint for rollout (not needed for training)
  
  # Model architecture
  action_encoder: null
  hub_dir: "/data/hub"
  dinov2_size: 224

  action_dim: 23
  action_encoder_hidden_dims: [1024]
  vae_compression_rate: 8
  text_cond: false
  frame_level_cond: true
  his_cond_zero: false
  num_views: 1
  only_wrist_view: false
  
  # Training/inference settings
  dtype: "bfloat16"  # Options: "float32", "float16", "bfloat16"
  hand_weight: 1.0
  motion_bucket_id: 127
  guidance_scale: 2.0
  num_inference_steps: 50
  decode_chunk_size: 10
  
  # Frame dimensions
  width: 256
  height: 256
  num_frames: 10
  num_history: 10