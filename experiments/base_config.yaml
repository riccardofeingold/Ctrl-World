# Base configuration that all experiments inherit from
training:
  debug: false
  learning_rate: 1e-5
  gradient_accumulation_steps: 1
  mixed_precision: "fp16"
  train_batch_size: 4
  shuffle: true
  num_train_epochs: 100
  max_train_steps: 100000
  checkpointing_steps: 10000
  validation_steps: 2500
  max_grad_norm: 1.0
  video_num: 4
  num_validation_batch: 2

dataset:
  dataset_root_path: "dataset_example"
  dataset_meta_info_path: "dataset_meta_info"
  annotation_name: "annotation"
  original_fps: 50
  num_workers: 4
  skip_step: 1
  max_num_samples: 13000
  prob: [1.0]

model:
  svd_model_path: "stabilityai/stable-video-diffusion-img2vid"
  clip_model_path: "openai/clip-vit-base-patch32"
  ckpt_path: null
  pi_ckpt: null
  
  # Model architecture
  action_dim: 23
  action_encoder_hidden_dims: [1024]
  vae_compression_rate: 8
  text_cond: false
  frame_level_cond: true
  his_cond_zero: false
  num_views: 1
  only_wrist_view: false
  
  # Training/inference settings
  dtype: "bfloat16"
  hand_weight: 2.5
  motion_bucket_id: 127
  guidance_scale: 2.0
  num_inference_steps: 50
  decode_chunk_size: 5
  
  # Frame dimensions
  width: 256
  height: 256
  num_frames: 5
  num_history: 5
